---
title: <span style="color:#034a94"> **Estimación de parámetros**</span>
author: "Modelos Estadisticos para la toma de decisiones"
output: html_document
css: style.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NA)
# colores ----------------------------------------------------------------
c1="#FF7F00"
c2="#=EB0C6"
c3="#034A94"
c4="#686868"
#-------------------------------------------------------------------------
gen.corr.data<- function(rho,n){
x <- rnorm(n)
z <- rnorm(n)
y<- rho*x + sqrt(1-rho^2)*z
result <-cbind(y,x)
return(result)
}
#-------------------------------------------------------------------------
library(paqueteMOD)
library(ggplot2)
library(patchwork)
Theme1= theme(axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks = element_blank(),
        axis.title.x = element_text(size = 12),
        axis.title.y = element_text(size = 12))
#------------------------------------------------------------------------
Theme2= theme(
        #axis.text.x = element_blank(),
        #axis.text.y = element_blank(),
        #axis.ticks = element_blank(),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11))

```

```{r, echo=FALSE, out.width="100%", fig.align = "center"}
# knitr::include_graphics("img/puntos1.png")
```


</br></br>

## <span style="color:#034a94">**Estimación por máxima verosimilitud (ML)**</br>

</br>

El método de mínimos cuadrados produce los mejores estimadores lineales insesgados para los parámetros de la recta y puede ser usado para la estimación de parámetros de un modelo de regresión lineal sin consideraciones distribucionales sobre los errores.

Sin embargo, para poder aplicar pruebas de hipótesis y construir intervalos de confianza, es necesario realizar y validar tales supuestos. Considerando para el modelo de regresión lineal simple los supuestos de normalidad, independencia y varianza constante para los errores, podemos usar el método de estimación de máxima verosimilitud (MLE).

Sean ${\left(x_1,y_1\right),\ldots,\left(x_n,y_n\right)}$ los ${n}$ pares de datos observados, entonces el modelo de regresión lineal simple es:
$$
{Y_i = Y\vert X_i=\beta_0+\beta_1X_i+\varepsilon_i,\ i=1,2,\ldots,n.}
$$


A la variable aleatoria ${\varepsilon_i}$, se le asignan los siguientes supuestos distribucionales:
$$
{\varepsilon_i\overset{\text{iid}}{\sim} N\left(0,\sigma^2\right),\ i=1,2,\ldots,n,}
$$
  
Con base en lo anterior y asumiendo que los niveles o valores en que ${X}$ es observada son fijos, se obtiene que $${Y_i = Y\vert X_i \overset{\text{ind}}{\sim} N(E\left[Y\vert X_i\right],\sigma^2)}$$
con
$${E\left[Y\vert X_i\right]=\beta_0+\beta_1X_i}.$$

Sean ${\textbf{x}=(x_1,x_2,\ldots,x_n)}$ y ${\textbf{y}=(y_1,y_2,\ldots,y_n)}$, entonces la función de verosimilitud ${L\left.(\beta _0,\beta_1,\sigma^2 \right|\textbf{x,y})}$ es hallada a partir de la densidad conjunta de las observaciones, ${f\left.(y_1,\dots,y_n\right|\beta_0,\beta_1,\sigma^2)}$, que por la condición de independencia es igual al producto de las densidades de probabilidad marginales, por tanto, podemos escribir,


$$
\begin{aligned}
{L(\beta_0,\beta_1,\sigma^2\vert \textbf{x,y})}&{=f(y_1,\ldots,y_n\vert\beta_0,\beta_1,\sigma^2)}\nonumber\\
&{=\prod\limits_{i=1}^n\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left[-\frac{1}{2\sigma^2}(y_i-\beta_0-\beta_1x_i)^2\right]}\nonumber\\
&{=(2\pi\sigma^2)^{-n/2}\exp\left[-\frac{1}{2\sigma^2}\sum\limits_{i=1}^n(y_i-\beta_0-\beta_1x_i)^2\right]}
\end{aligned}
$$
</br>

El objetivo es hallar los parámetros desconocidos ${\beta_0,\beta_1,\sigma^2}$, que maximicen ${L}$, o equivalentemente, que maximicen ${\ln L}$ (el logaritmo natural de ${L}$). 
$$
{\ln L=-\frac{n}{2}\ln(2\pi)-\frac{n}{2}\ln(\sigma^2)-\frac{1}{2\sigma^2}\sum\limits_{i=1}^n \big(y_i-\beta_0-\beta_1x_i\big)^2}
$$

 </br>
 
Observe que para cualquier valor de ${\sigma^2}$ fijo, ${\ln L}$ es maximizado como una función de ${\beta_0}$ y ${\beta_1}$ por aquellos valores ${\widetilde{\beta}_0}$ y ${\widetilde{\beta}_1}$ que minimizan ${S(\beta_0,\beta_1)=\sum\limits_{i=1}^n(y_i-\beta_0-\beta_1x_i)^2}$ y así, los estimadores MLE $\ {\widetilde{\beta}_0}$ y ${\widetilde{\beta}_1}$ son iguales a los respectivos estimadores de mínimos cuadrados, ${\widehat{\beta}_0}$ y ${\widehat{\beta}_1}$.


</br>

Para hallar el estimador MLE para ${\sigma^2}$ substituimos ${\widehat{\beta}_0}$ y ${\widehat{\beta}_1}$ en ${\ln L}$, y hallamos ${\sigma^2}$ que maximiza

$$
{-\frac{n}{2}\ln(2\pi) - \frac{n}{2}\ln(\sigma^2) - \frac{1}{2\sigma^2}\sum\limits_{i=1}^n (y_i - \widehat{\beta}_0 - \widehat{\beta}_1x_i)^2}
$$


de donde obtenemos como estimador MLE de ${\sigma^2}$ a

$$
{\widetilde{\sigma}^2 = \frac{1}{n}\sum\limits_{i=1}^n (y_i - \widehat{\beta}_0 - \widehat{\beta}_1x_i)^2 = \frac{1}{n}\sum\limits_{i=1}^n (y_i - \widehat{y}_i)^2}
$$

</br>

Resumiendo, bajo el modelo de regresión lineal normal, es decir, con errores independientes e idénticamente distribuidos ${N\left(0, \sigma^2\right)}$, los estimadores de mínimos cuadrados para ${\beta_0}$ y ${\beta_1}$ son también estimadores de máxima verosimilitud y en tal caso, podemos construir intervalos de confianza y realizar pruebas de hipótesis basadas en las estimaciones obtenidas.

</br>

También puede demostrarse que los estimadores MLE son de mínima varianza cuando son comparados con todos los posibles estimadores insesgados y son consistentes, es decir, a medida que aumenta el tamaño de muestra, la diferencia entre éstos y los respectivos parámetros se aproxima a cero.


</br></br>

## <span style="color:#034a94">**Estimación de la varianza $\sigma^2$**</span>

</br>

Puede demostrarse que bajo los supuestos del modelo en relación a los errores, la esperanza del estimador de máxima verosimilitud de $\sigma^2$ es:
$$
{E\left[\widetilde{\sigma}^2\right] = \left(\frac{n - 2}{n}\right) \sigma^2}
$$
  
por tanto $\widetilde{\sigma}^2$ no es un estimador insesgado de $\sigma^2$, aunque si es asintóticamente insesgado, esto es, ${\lim\limits_{n\to\infty} E\left[\widetilde\sigma^2\right] = \sigma^2}$. Sin embargo, a partir de $\widetilde{\sigma}^2$ se puede obtener un estimador insesgado de la varianza, así:
$$
{\widehat{\sigma}^2 = \left(\frac{n}{n - 2}\right) \widetilde{\sigma}^2 = \frac{\sum\limits_{i=1}^n (y_i - \widehat{y}_i)^2}{n-2}}
$$

 
que cumple ${E\left[\widehat\sigma^2\right]=\sigma^2}$.

</br></br>

## <span style="color:#034a94">**Ecuación de regresión ajustada y residuales del modelo **</span>

</br>

Al tener estimados los parámetros del modelo de regresión lineal simple (por mínimos cuadrados o máxima verosimilitud), entonces se puede realizar una estimación de la  respuesta media $E\left[Y \vert X\right] = \mu_{Y\vert X}$, a través del modelo ajustado, así:
$$
\widehat{\mu}_{Y\vert x_i} = \widehat{y}_i = \widehat{\beta}_0 + \widehat{\beta}_1\, x_i = \bar{y} + (x_i-\bar{x})\,\widehat{\beta}_1.
$$

</br>

A esta ecuación se le conoce como la ecuación de regresión ajustada, que en este caso corresponde a una recta ajustada.

</br>

A las diferencias entre los valores observados de la respuesta $y_i$ y los valores ajustados por el modelo de regresión $\widehat{y}_i$ (obtenidos de la ecuación de regresión ajustada) se les conoce como los residuales del modelo. Esto es, ${e_i=y_i-\widehat{y}_i}$ es el $i$-ésimo residual del modelo, que es una estimación del $i$-ésimo error aleatorio, $\varepsilon_i$.

</br>

Los residuales del modelo tienen gran importancia ya que ellos determinan que tan bueno fue el ajuste del modelo y permitirán más adelante realizar las validaciones de los supuestos realizados sobre los errores aleatorios.

</br></br>

