---
title: <span style="color:#034a94"> **Inferencia sobre $Y_o$**</span>
author: "Modelos Estadisticos para la toma de decisiones"
output: html_document
css: style.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NA)
if(!require(pacman)){install.packages("pacman"); library(pacman)}
pacman::p_load("tidyverse", "ggplot2", "pBrackets", "knitr", "HH", "car", "rgl", "sampling")

c1="#FF7F00"
c2="#=EB0C6"
c3="#034A94"
c4="#686868"


```

</br></br>

## **Inferencias respecto a Y**

En ocaciones interesa realizar inferencias sobre la respuesta, para un valor apropiado $X = x_0$, así:

* Estimación puntual y por intervalo de la respuesta media $E[Y|x_0]$.

* Predicción de valores futuros $y_0 = \beta_0 + \beta_1 x_0 + \varepsilon_0 = E[Y\vert x_0] + \varepsilon_0$.

En ambos casos el único medio para producir tales inferencias es la ecuación de regresión ajustada.

</br>

Conociendo  que la ecuación de regresión ajustada, en un valor dado ${X = x_0}$, es:

$$\widehat{Y}_0 = \widehat\beta_0 + \widehat\beta_1x_0$$
</br>

Note que ${\widehat{Y}_0}$ también es una combinación lineal de las variables aleatorias ${Y_1,\ldots ,Y_n}$. En efecto,

$$
\widehat{Y}_0 = \widehat\beta_0 + \widehat\beta_1x_0 = \left(\sum^n_{i=1} m_iY_i\right) + \left(\sum^n_{i=1} c_iY_i\right)x_0 = \sum^n_{i=1} (m_i + x_0\, c_i) Y_i,
$$

</br>

con las constantes $m_i = \frac{1}{n} - \bar{x}\, c_i$ y $c_i = \frac{x_i - \bar{x}}{S_{xx}}$ como fueron especificadas previamente. Por lo tanto, bajo los supuestos del modelo ${\widehat{Y}_0}$ es una variable aleatoria normal.

con esperanza:

$$
E\left[\widehat{Y}_0\right] = E\left[\widehat\beta_0 + \widehat\beta_1x_0\right] = E\left[\widehat\beta_0\right] + E\left[\widehat\beta_1\right]x_0 = \beta_0 + \beta_1x_0 = E[Y\vert x_0]
$$
</br>

y su varianza:

$$
\begin{aligned}
{V\left[\widehat{Y}_0\right]} &= {V\left[\sum^n_{i=1} (m_i + x_0\, c_i) Y_i\right]} = {\sum^n_{i=1} (m_i + x_0\, c_i)^2\ V(Y_i)}\nonumber\\
&= {\sigma^2\sum^n_{i=1} \left[\left(\frac{1}{n} - \bar{x}c_i\right)+x_0c_i\right]^2} = {\sigma^2\sum^n_{i=1} \left[\frac{1}{n} + (x_0-\bar{x}) c_i\right]^2}\nonumber\\
&= {\sigma^2\left[\frac{1}{n} + \frac{(x_0-\bar{x})^2}{S_{xx}}\right]}
\end{aligned}
$$

</br></br>

En resumen,
$$
\widehat{Y}_0 \sim N\left(E[Y\vert x_0], \ \sigma^2\left[\frac{1}{n} + \frac{(x_0 - \bar{x})^2}{S_{xx}}\right]\right)
$$

Esto es, $\widehat{Y}_0$ es un estimador insesgado de la respuesta media.

Note que, $\widehat{Y}_0$ también es un estimador para un valor futuro $y_0$, pero en este caso es un estimador sesgado. De ahí que la cantidad $y_0 - \widehat{Y}_0$ represente al error de predicción, el cual se sabe tiene media cero.



Tanto las estimaciones de valores de la respuesta media como las predicciones de valores futuro deben cumplir una condición sobre el valor fijo ${X = x_0}$ para que tal estimación/predicción sea válida.

* Sólo se podrán hacer inferencias sobre la respuesta cuando $X = x_0 \in \left[X_{\min}, X_{\max}\right]$, donde $X_{\min}$ y $X_{\max}$ son los valores mínimo y máximo de la variable predictora, que fueron fijados en la muestra.

* Cumplir con lo anterior indica que $x_0$ es un punto de interpolación.

* Esto evita que $x_0$ sea un punto de extrapolación, esto es, un punto por fuera del rango experimental donde el modelo fue ajustado y que no garantiza que el modelo se mantenga.

---

```{r, include = F}
dfe <- NULL
dfe$x <- rep(seq(100, 140, 5), 4)
set.seed(3)
dfe$y <- 250 + 0.5*(dfe$x - 150)^2 + 0.0053*(dfe$x - 150)^3 + rnorm(36, sd = 35)
lm1 <- summary(lm(y ~ x, dfe))
```

**Ilustración de puntos de interpolación y extrapolación**

```{r, echo = F, fig.align = 'center', fig.show = "hold", out.width = "90%"}
par(mar = c(4, 4, .1, .1))
plot(0, 0, xlim = c(70, 170), ylim = c(0, 1250), xaxt = 'n', yaxt = 'n',
      xlab = '', ylab = '')
rect(0, -100, 200, 1350, col = 'lightgray', border = NA, density = -10)
rect(100, -100, 140, 1350, col = 'lightblue', border = NA, density = -10)
box()
par(new = T)
with(dfe, {plot(x, y, pch = 19, cex = 1.2, xlim = c(70, 170), ylim = c(0, 1250), xaxt = 'n', yaxt = 'n', cex.lab = 2)
   axis(1, at = seq(70, 170, 20), mgp = c(3, 1.5, 0), cex.axis = 2, cex.lab = 2)
   axis(2, at = seq(0, 1200, 300), mgp = c(3, 1, 0), cex.axis = 2, cex.lab = 2)
   abline(lm1$coefficients[,1], col = 'red', lwd = 2)})
par(new = T)
curve(250 + 0.5*(x-150)^2 + 0.0053*(x-150)^3, 40, 200, xlim = c(70, 170), ylim = c(0, 1250), xaxt = 'n', yaxt = 'n',
      xlab = '', ylab = '', col = 'blue', lwd = 2)
mtext(c(expression(x['01'] == 80), expression(x['02'] == 122), expression(x['03'] == 160)), side = 1, at = c(80, 122, 160), line = 0.8, cex = 1.5)
new <- data.frame(x = c(80, 122, 160))
pr <- as.vector(predict(lm(y ~ x, dfe), newdata = new))
segments(x0 = c(80, 122, 160), y0 = -100, x1 = c(80, 122, 160), y1 = pr, lwd = 1.5)
text(120, 1200, "Rango experimental", col = 'darkblue', cex = 2)
text(82.5, 600, "Zona de", col = 'red', cex = 1.5)
text(82.5, 550, "extrapolación", col = 'red', cex = 1.5)
text(157.5, 900, "Zona de", col = 'red', cex = 1.5)
text(157.5, 850, "extrapolación", col = 'red', cex = 1.5)
brackets(80, 882.1, 80, 1126.9, h = 3, col = 'brown', lwd = 2)
brackets(160, 305.3, 160, -12.1, h = 2, col = 'brown', lwd = 2)
text(72.5, mean(c(882.1, 1126.9)), "error", col = 'brown', cex = 1.5)
text(167.5, mean(c(305.3, -12.1)), "error", col = 'brown', cex = 1.5)
```

## Intervalo de confianza para la respuesta media

Se puede demostrar que bajo los supuestos del modelo:
$$
{T = \frac{\widehat{y}_0 - E[Y\vert x_0]}{\sqrt{\widehat{\sigma}^2\left[\frac{1}{n} + \frac{(x_0 - \bar{x})^2}{S_{xx}}\right]}} \sim t_{n - 2}}
$$

Por tanto un intervalo de confianza del ${(1-\alpha)\%}$ para ${\mu_{Y\vert x_0}}$ es:
$$
{\widehat{y}_{0} \pm t_{\alpha/2,n-2}\times\sqrt{\widehat{\sigma}^2\left[\frac{1}{n} + \frac{(x_{0}-\bar{x})^{2}}{S_{xx}}\right]}}
$$
  
con ${\widehat{y}_{0}=\widehat{\beta}_0+\widehat{\beta}_1x_0}$ y ${t_{\alpha/2, n - 2}}$ es el percentil ${1 - \alpha/2}$ de la distribución ${t}$-Student con ${n - 2}$ grados de libertad.

## Intervalo de predicción para una observación futura de la respuesta $\boldsymbol{Y_0}$

Dicho intervalo estima los posibles valores para un valor particular de la variable respuesta (no para su media) en un valor dado, ${X=x_0}$. Asumimos que este valor particular es un valor futuro de la variable aleatoria ${Y}$ y por tanto, no fue utilizado en la regresión.

Si ${Y_0}$ es un valor futuro y ${\widehat{y}_0 = \widehat{\beta}_0 + \widehat{\beta}_1x_0}$ es su estimador, entonces estas dos variables aleatorias son estadísticamente independientes, dado que ${Y_0}$ no es utilizado para hallar a ${\widehat{\beta}_0}$ y ${\widehat{\beta}_1}$.

---

Por tanto, el estadístico:
$$
{T=\frac{\widehat{y}_0-Y_0}{\sqrt{\widehat{\sigma}^2\left[1+\frac{1}{n}+\frac{(x_{0}-\bar{x})^2}{S_{xx}}\right]}} \sim t_{n-2}}
$$

De ahí que, un intervalo de predicción del ${(1 - \alpha)\%}$ para ${Y_0}$ está dado por:
$$
{\widehat{y}_0\pm t_{\alpha/2,n-2}\times\sqrt{\widehat{\sigma}^2\left[1+\frac{1}{n}+\frac{(x_0-\bar{x})^2}{S_{xx}}\right]}}
$$

Donde, ${t_{\alpha/2, n - 2}}$ es el percentil ${1 - \alpha/2}$ de la distribución ${t}$-Student con ${n - 2}$ grados de libertad.

## Pruebas de hipótesis para la respuesta media

Para la respuesta media se pueden probar hipótesis a partir de la construcción y el análisis de los intervalos de confianza definidos anteriormente. Esto es, para probar a un nivel de significancia $\alpha$, el siguiente juego de hipótesis:
$$
\begin{aligned}
{H_0:\ E[Y\vert x_0] = c_0}\nonumber\\
{H_1:\ E[Y\vert x_0] \neq c_0}\nonumber
\end{aligned}
$$

Donde $c_0 \in \mathbb{R}$, se calcula un intervalo de confianza del $(1 - \alpha)100$% para $E[Y\vert x_0]$ y si el valor $c_0$ está incluido en el intervalo, entonces no se rechaza $H_0$, o si el valor $c_0$ no está incluido en el intervalo, entonces se rechaza $H_0$.

