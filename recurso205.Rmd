---
title: <span style="color:#034a94"> **Inferencia sobre $Y_o$**</span>
author: "Modelos Estadisticos para la toma de decisiones"
output: html_document
css: style.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NA)
if(!require(pacman)){install.packages("pacman"); library(pacman)}
pacman::p_load("tidyverse", "ggplot2", "pBrackets", "knitr", "HH", "car", "rgl", "sampling")

c1="#FF7F00"
c2="#=EB0C6"
c3="#034A94"
c4="#686868"


```

</br></br>

En ocaciones interesa realizar inferencias sobre la respuesta, para un valor apropiado $X = x_0$, así:

* Estimación puntual y por intervalo de la respuesta media $E[Y|x_0]$.

* Predicción de valores futuros $y_0 = \beta_0 + \beta_1 x_0 + \varepsilon_0 = E[Y\vert x_0] + \varepsilon_0$.

</br>

En ambos casos el único medio para producir tales inferencias es la ecuación de regresión ajustada.

</br>

Conociendo  que la ecuación de regresión ajustada, en un valor dado ${X = x_0}$, es:

$$\widehat{Y}_0 = \widehat\beta_0 + \widehat\beta_1x_0$$
</br>

Note que ${\widehat{Y}_0}$ también es una combinación lineal de las variables aleatorias ${Y_1,\ldots ,Y_n}$. En efecto,

$$
\widehat{Y}_0 = \widehat\beta_0 + \widehat\beta_1x_0 = \left(\sum^n_{i=1} m_iY_i\right) + \left(\sum^n_{i=1} c_iY_i\right)x_0 = \sum^n_{i=1} (m_i + x_0\, c_i) Y_i,
$$

</br>

con las constantes $m_i = \frac{1}{n} - \bar{x}\, c_i \hspace{.4cm} \text{y} \hspace{.4cm} c_i = \frac{x_i - \bar{x}}{S_{xx}}$ como fueron especificadas previamente. Por lo tanto, bajo los supuestos del modelo ${\widehat{Y}_0}$ es una variable aleatoria normal.

con esperanza:

$$
E\left[\widehat{Y}_0\right] = E\left[\widehat\beta_0 + \widehat\beta_1x_0\right] = E\left[\widehat\beta_0\right] + E\left[\widehat\beta_1\right]x_0 = \beta_0 + \beta_1x_0 = E[Y\vert x_0]
$$
</br>

y su varianza:

$$
\begin{aligned}
{V\left[\widehat{Y}_0\right]} &= {V\left[\sum^n_{i=1} (m_i + x_0\, c_i) Y_i\right]} = {\sum^n_{i=1} (m_i + x_0\, c_i)^2\ V(Y_i)}\nonumber\\
&= {\sigma^2\sum^n_{i=1} \left[\left(\frac{1}{n} - \bar{x}c_i\right)+x_0c_i\right]^2} = {\sigma^2\sum^n_{i=1} \left[\frac{1}{n} + (x_0-\bar{x}) c_i\right]^2}\nonumber\\
&= {\sigma^2\left[\frac{1}{n} + \frac{(x_0-\bar{x})^2}{S_{xx}}\right]}
\end{aligned}
$$

</br></br>

<div class="content-box-gray">

### <span style="color:#686868">**Resumen**</span>

$$
\widehat{Y}_0 \sim N\left(E[Y\vert x_0], \ \sigma^2\left[\frac{1}{n} + \frac{(x_0 - \bar{x})^2}{S_{xx}}\right]\right)
$$

</div>

</br></br>


Esto es, $\widehat{Y}_0$ es un estimador insesgado de la respuesta media.

Note que, $\widehat{Y}_0$ también es un estimador para un valor futuro $y_0$, pero en este caso es un estimador sesgado. De ahí que la cantidad $y_0 - \widehat{Y}_0$ represente al error de predicción, el cual se sabe tiene media cero.

</br>

Tanto las estimaciones de valores de la respuesta media como las predicciones de valores futuro deben cumplir una condición sobre el valor fijo ${X = x_0}$ para que tal estimación/predicción sea válida.

</br>

* Sólo se podrán hacer inferencias sobre la respuesta cuando $X = x_0 \in \left[X_{\min}, X_{\max}\right]$, donde $X_{\min}$ y $X_{\max}$ son los valores mínimo y máximo de la variable predictora, que fueron fijados en la muestra.

</br>

* Cumplir con lo anterior indica que $x_0$ es un punto de interpolación.

</br>

* Esto evita que $x_0$ sea un punto de extrapolación, esto es, un punto por fuera del rango experimental donde el modelo fue ajustado y que no garantiza que el modelo se mantenga.



```{r, include = F}
dfe <- NULL
dfe$x <- rep(seq(100, 140, 5), 4)
set.seed(3)
dfe$y <- 250 + 0.5*(dfe$x - 150)^2 + 0.0053*(dfe$x - 150)^3 + rnorm(36, sd = 35)
lm1 <- summary(lm(y ~ x, dfe))
```

<center>


```{r, echo = F, fig.align = 'center', fig.show = "hold", out.width = "90%"}
par(mar = c(4, 4, .1, .1))
plot(0, 0, xlim = c(70, 170), ylim = c(0, 1250), xaxt = 'n', yaxt = 'n',
     xlab = '', ylab = '')
rect(0, -100, 200, 1350, col = '#E8E8E8', border = NA, density = -10)
rect(100, -100, 140, 1350, col = "#BABABA", border = NA, density = -10)
box()
par(new = T)
with(dfe, {plot(x, y, pch = 19, cex = 1.2, xlim = c(70, 170), ylim = c(0, 1250), xaxt = 'n', yaxt = 'n', cex.lab = 1)
  axis(1, at = seq(70, 170, 20), mgp = c(3, 1.5, 0), cex.axis = 1, cex.lab = 1)
  axis(2, at = seq(0, 1200, 300), mgp = c(3, 1, 0), cex.axis = 1, cex.lab = 1)
  abline(lm1$coefficients[,1], col = 'red', lwd = 2)})
par(new = T)
curve(250 + 0.5*(x-150)^2 + 0.0053*(x-150)^3, 40, 200, xlim = c(70, 170), ylim = c(0, 1250), xaxt = 'n', yaxt = 'n',
      xlab = '', ylab = '', col = 'blue', lwd = 2,  )
mtext(c(expression(x['1'] == 80), expression(x['2'] == 122), expression(x['3'] == 160)), side = 1, at = c(80, 122, 160), line = 0.8, cex = 1)
new <- data.frame(x = c(80, 122, 160))
pr <- as.vector(predict(lm(y ~ x, dfe), newdata = new))
segments(x0 = c(80, 122, 160), y0 = -100, x1 = c(80, 122, 160), y1 = pr, lwd = 1, col="#686868",lty = 2)
text(120, 1200, "Rango experimental", col = 'darkblue', cex = 2)
text(89, 600, "Zona de", col = 'red', cex = 1)
text(89, 540, "extrapolación", col = 'red', cex = 1)
text(157.5, 900, "Zona de", col = 'red', cex = 1)
text(157.5, 840, "extrapolación", col = 'red', cex = 1)
brackets(80, 882.1, 80, 1126.9, h = 3, col = 'brown', lwd = 1)
brackets(160, 305.3, 160, -12.1, h = 2, col = 'brown', lwd = 1)
text(72.5, mean(c(882.1, 1126.9)), "error", col = 'brown', cex = 1)
text(167.5, mean(c(305.3, -12.1)), "error", col = 'brown', cex = 1)
```

Figura : Ilustración de puntos de interpolación y extrapolación

</center>

</br></br>

## <span style="color:#034a94">**Intervalo de confianza para la respuesta media**</span>

</br>

Se puede demostrar que bajo los supuestos del modelo:

</br>

$$
{T = \frac{\widehat{y}_0 - E[Y\vert x_0]}{\sqrt{\widehat{\sigma}^2\left[\frac{1}{n} + \frac{(x_0 - \bar{x})^2}{S_{xx}}\right]}} \sim t_{n - 2}}
$$

</br>

Por tanto un intervalo de confianza del ${(1-\alpha)\%}$ para ${\mu_{Y\vert x_0}}$ es:

</br>

<div class="content-box-blue">
$$
{\widehat{y}_{0} \pm t_{\alpha/2,n-2}\times\sqrt{\widehat{\sigma}^2\left[\frac{1}{n} + \frac{(x_{0}-\bar{x})^{2}}{S_{xx}}\right]}}
$$
</div>

</br>

con ${\widehat{y}_{0}=\widehat{\beta}_0+\widehat{\beta}_1x_0}$ y ${t_{\alpha/2, n - 2}}$ es el percentil ${1 - \alpha/2}$ de la distribución ${t}$-Student con ${n - 2}$ grados de libertad.

</br></br>

## <span style="color:#034a94">**Intervalo de predicción para una observación futura de la respuesta $Y_{0}$**</span>

</br>

Dicho intervalo estima los posibles valores para un valor particular de la variable respuesta (no para su media) en un valor dado, ${X=x_0}$. Asumimos que este valor particular es un valor futuro de la variable aleatoria ${Y}$ y por tanto, no fue utilizado en la regresión.

</br>

Si ${Y_0}$ es un valor futuro y ${\widehat{y}_0 = \widehat{\beta}_0 + \widehat{\beta}_1x_0}$ es su estimador, entonces estas dos variables aleatorias son estadísticamente independientes, dado que ${Y_0}$ no es utilizado para hallar a ${\widehat{\beta}_0}$ y ${\widehat{\beta}_1}$.

</br>

Por tanto, el estadístico:

</br>

$$
{T=\frac{\widehat{y}_0-Y_0}{\sqrt{\widehat{\sigma}^2\left[1+\frac{1}{n}+\frac{(x_{0}-\bar{x})^2}{S_{xx}}\right]}} \sim t_{n-2}}
$$

</br>

De ahí que, un intervalo de predicción del ${(1 - \alpha)\%}$ para ${Y_0}$ está dado por:

</br>

<div class="content-box-blue">
$$
{\widehat{y}_0\pm t_{\alpha/2,n-2}\times\sqrt{\widehat{\sigma}^2\left[1+\frac{1}{n}+\frac{(x_0-\bar{x})^2}{S_{xx}}\right]}}
$$
</div>

</br>

Donde, ${t_{\alpha/2, n - 2}}$ es el percentil ${1 - \alpha/2}$ de la distribución ${t}$-Student con ${n - 2}$ grados de libertad.

</br></br>

## <span style="color:#034a94">**Pruebas de hipótesis para la respuesta media**</span>

</br>

Para la respuesta media se pueden probar hipótesis a partir de la construcción y el análisis de los intervalos de confianza definidos anteriormente. Esto es, para probar a un nivel de significancia $\alpha$, el siguiente juego de hipótesis:

</br>

$$
\begin{aligned}
{H_0:\ E[Y\vert x_0] = c_0}\nonumber\\
{H_1:\ E[Y\vert x_0] \neq c_0}\nonumber
\end{aligned}
$$

</br>

Donde $c_0 \in \mathbb{R}$, se calcula un intervalo de confianza del $(1 - \alpha)100$% para $E[Y\vert x_0]$ y si el valor $c_0$ está incluido en el intervalo, entonces no se rechaza $H_0$, o si el valor $c_0$ no está incluido en el intervalo, entonces se rechaza $H_0$.

